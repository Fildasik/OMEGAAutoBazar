import requests
from bs4 import BeautifulSoup
import time
import pandas as pd
<<<<<<< HEAD
import os
from concurrent.futures import ThreadPoolExecutor, as_completed

# -----------------------------------------------------------
# 1) TÅ˜ÃDA CSVManager: starÃ¡ se o sloÅ¾ku data/ a o CSV soubor
# -----------------------------------------------------------

class CSVManager:
    def __init__(self, csv_file):
        """
        Konstruktor tÅ™Ã­dy CSVManager.
        csv_file: cesta k CSV souboru, napÅ™. "data/auta_aaaauto.csv"
        """
        self.csv_file = csv_file
        # Pokud neexistuje adresÃ¡Å™, ve kterÃ©m csv_file je, vytvoÅ™Ã­me ho
        os.makedirs(os.path.dirname(self.csv_file), exist_ok=True)

    def load_existing_urls(self):
        """
        NaÄte existujÃ­cÃ­ URL z CSV, pokud soubor existuje.
        VrÃ¡tÃ­ mnoÅ¾inu URL.
        """
        if os.path.exists(self.csv_file):
            try:
                df = pd.read_csv(self.csv_file)
                # Pokud by v CSV nebyl sloupec URL (teÄ tam nebude),
                # pak staÄÃ­ vrÃ¡tit prÃ¡zdnou mnoÅ¾inu.
                if "URL" in df.columns:
                    return set(df["URL"].unique())
            except Exception as e:
                print(f"Chyba pÅ™i naÄÃ­tÃ¡nÃ­ existujÃ­cÃ­ch dat: {e}")
        return set()

    def save_ads(self, ads):
        """
        UloÅ¾Ã­ novÃ© zÃ¡znamy (ads) do CSV souboru.
        Pokud CSV uÅ¾ existuje, spojÃ­ starÃ© a novÃ© zÃ¡znamy a odstranÃ­ duplicity
        podle hodnot sloupcÅ¯, kterÃ© jsou v ads.
        """
        if not ads:
            return

        df_new = pd.DataFrame(ads)
        if os.path.exists(self.csv_file):
            df_existing = pd.read_csv(self.csv_file)
            # Sloupec URL uÅ¾ v ads nebude, takÅ¾e klidnÄ› vynechÃ¡me deduplikaci
            # podle URL. MÅ¯Å¾eÅ¡ ale zvolit jinÃ½ klÃ­Ä, pokud ho tam mÃ¡Å¡.
            # Pro ukÃ¡zku deduplikaci vypneme (nebo deduplikuj podle 'Model', atd.)
            df_combined = pd.concat([df_existing, df_new], ignore_index=True)
            # df_combined.drop_duplicates(subset=["Model", "Rok"], inplace=True)
        else:
            df_combined = df_new

        df_combined.to_csv(self.csv_file, index=False)

# -----------------------------------------------------------
# 2) HLAVNÃ LOGIKA SCRAPERU
# -----------------------------------------------------------

CSV_FILE = "data/auta_aaaauto.csv"
MAX_ADS = 1000
NUM_PAGES = 100

csv_manager = CSVManager(CSV_FILE)
existing_urls = csv_manager.load_existing_urls()

def is_complete(ad):
    required_keys = ["ZnaÄka", "Model", "Rok", "NajetÃ© km", "Cena", "Palivo", "PÅ™evodovka", "VÃ½kon (kW)"]
    for key in required_keys:
        if ad.get(key, "NezjiÅ¡tÄ›no") == "NezjiÅ¡tÄ›no":
            return False
    return True

def print_ad_no_url(ad):
    """
    VypÃ­Å¡e inzerÃ¡t do terminÃ¡lu "hezky pod sebou" bez URL.
    """
    print("========================================")
    for key, value in ad.items():
        print(f"{key}: {value}")
    print("========================================\n")

def clean_numeric(value):
    return value.replace("kW", "").strip()

import re

def fetch_url(url, session, max_retries=3, timeout=30):
    for attempt in range(1, max_retries + 1):
        try:
            response = session.get(url, timeout=timeout)
=======
import concurrent.futures

pd.set_option('display.max_colwidth', None)

headers = {
    "User-Agent": "Mozilla/5.0"
}

def fetch_url(url, headers, max_retries=3, timeout=30):
    """
    NaÄte URL s opakovanÃ½mi pokusy, aby se minimalizovaly chyby s timeoutem.
    """
    for attempt in range(1, max_retries + 1):
        try:
            response = requests.get(url, headers=headers, timeout=timeout)
>>>>>>> bb487f5469bd694df803453a80042a5bee964868
            response.raise_for_status()
            return response
        except Exception as e:
            print(f"Chyba pÅ™i naÄÃ­tÃ¡nÃ­ {url} (pokus {attempt}/{max_retries}): {e}")
<<<<<<< HEAD
            time.sleep(0.5)
    return None

import requests

session = requests.Session()
session.headers.update({
    "User-Agent": "Mozilla/5.0"
})

def get_listing_links_from_pages(base_url, num_pages=5, session=None):
    all_links = []
    for page in range(1, num_pages+1):
        url = f"{base_url}?page={page}"
        try:
            response = session.get(url, timeout=10)
            response.raise_for_status()
        except Exception as e:
            print(f"Chyba pÅ™i naÄÃ­tÃ¡nÃ­ listingu {url}: {e}")
            continue
        soup = BeautifulSoup(response.text, "html.parser")
        links = []
        for a in soup.find_all("a", href=True):
            href = a["href"]
            if "car.html" in href:
                if not href.startswith("http"):
                    href = "https://www.aaaauto.cz" + href
                links.append(href)
        all_links.extend(links)
        time.sleep(0.5)
    return list(set(all_links))

def parse_aaaauto_detail(url, session):
    """
    VracÃ­ slovnÃ­k BEZ klÃ­Äe 'URL'.
    """
    # Tohle si drÅ¾Ã­me jen kvÅ¯li kontrole duplicity
    if url in existing_urls:
        return None

    response = fetch_url(url, session)
    if not response:
        return None

    soup = BeautifulSoup(response.text, "html.parser")

    # PÅ™ipravÃ­me si slovnÃ­k s daty (bez URL)
    details_no_url = {
=======
            if attempt < max_retries:
                time.sleep(2)
    return None

def get_listing_links(listing_url):
    """
    NaÄte strÃ¡nku s inzerÃ¡ty a najde vÅ¡echny odkazy obsahujÃ­cÃ­ 'car.html'.
    """
    response = fetch_url(listing_url, headers=headers)
    if not response:
        return []
    soup = BeautifulSoup(response.text, "html.parser")
    links = []

    for a in soup.find_all("a", href=True):
        href = a["href"]
        if "car.html" in href:
            if not href.startswith("http"):
                href = "https://www.aaaauto.cz" + href
            links.append(href)

    return list(set(links))

def parse_aaaauto_detail(url):
    """
    NaÄte detail inzerÃ¡tu a extrahuje Ãºdaje:
      - ZnaÄka, Model, Rok, NajetÃ© km, Cena, Palivo, PÅ™evodovka, VÃ½kon (kW).
    """
    details = {
        "URL": url,
>>>>>>> bb487f5469bd694df803453a80042a5bee964868
        "ZnaÄka": "NezjiÅ¡tÄ›no",
        "Model": "NezjiÅ¡tÄ›no",
        "Rok": "NezjiÅ¡tÄ›no",
        "NajetÃ© km": "NezjiÅ¡tÄ›no",
        "Cena": "NezjiÅ¡tÄ›no",
        "Palivo": "NezjiÅ¡tÄ›no",
        "PÅ™evodovka": "NezjiÅ¡tÄ›no",
        "VÃ½kon (kW)": "NezjiÅ¡tÄ›no"
    }

<<<<<<< HEAD
    # 1) Cena
=======
    response = fetch_url(url, headers=headers)
    if not response:
        return details

    soup = BeautifulSoup(response.text, "html.parser")

    # 1) Cena â€“ pouze z hlavnÃ­ho prvku
>>>>>>> bb487f5469bd694df803453a80042a5bee964868
    price_el = soup.find("strong", class_="carCard__price-value carCard__price-value--big textGrey notranslate")
    if price_el:
        price_text = price_el.get_text(" ", strip=True)
        price_text = price_text.replace("KÄ", "").replace("\xa0", "").replace(" ", "").strip()
<<<<<<< HEAD
        details_no_url["Cena"] = price_text

    # 2) DalÅ¡Ã­ Ãºdaje
    li_tags = soup.find_all("li")
    found_automat = False
    for li in li_tags:
        text = li.get_text(" ", strip=True).lower()
        strong = li.find("strong")
        if strong:
            value = strong.get_text(strip=True)
            if "znaÄka" in text:
                details_no_url["ZnaÄka"] = value
            elif "model" in text:
                details_no_url["Model"] = value
            elif "rok" in text:
                details_no_url["Rok"] = value
            elif "tachometr" in text:
                details_no_url["NajetÃ© km"] = value
            elif "palivo" in text:
                details_no_url["Palivo"] = value
            elif "vÃ½kon" in text:
                details_no_url["VÃ½kon (kW)"] = clean_numeric(value)

        if "automat" in text:
            found_automat = True

    if found_automat:
        details_no_url["PÅ™evodovka"] = "Automat"
    else:
        details_no_url["PÅ™evodovka"] = "ManuÃ¡l"

    time.sleep(0.5)

    if is_complete(details_no_url):
        # Hezky vypÃ­Å¡eme do terminÃ¡lu
        print_ad_no_url(details_no_url)
        return details_no_url
    else:
        return None

if __name__ == "__main__":
    base_url = "https://www.aaaauto.cz/ojete-vozy/"  # PÅ™Ã­klad URL
    links = get_listing_links_from_pages(base_url, NUM_PAGES, session)
    # OmezÃ­me poÄet odkazÅ¯
    links = links[:MAX_ADS]
    print(f"Nalezeno {len(links)} odkazÅ¯ (limit {MAX_ADS}).")

    ads = []
    with ThreadPoolExecutor(max_workers=5) as executor:
        future_to_url = {executor.submit(parse_aaaauto_detail, url, session): url for url in links}
        for future in as_completed(future_to_url):
            ad = future.result()
            if ad:
                ads.append(ad)

    # UloÅ¾Ã­me do CSV (uÅ¾ bez sloupce URL)
    csv_manager.save_ads(ads)
=======
        details["Cena"] = price_text

    # 2) HlavnÃ­ data z <li>
    li_tags = soup.find_all("li")
    for li in li_tags:
        text = li.get_text(" ", strip=True)
        strong = li.find("strong")
        if not strong:
            continue
        value = strong.get_text(strip=True)

        if "ZnaÄka" in text:
            details["ZnaÄka"] = value
        elif "Model" in text:
            details["Model"] = value
        elif "Rok" in text:
            details["Rok"] = value
        elif "Tachometr" in text:
            details["NajetÃ© km"] = value.replace("km", "").strip()
        elif "Palivo" in text:
            details["Palivo"] = value
        elif "PÅ™evodovka" in text:
            details["PÅ™evodovka"] = value
        elif "VÃ½kon" in text:
            details["VÃ½kon (kW)"] = value.replace("kW", "").strip()

    # 3) Fallback â€“ pokud znaÄka/model/rok atd. stÃ¡le NezjiÅ¡tÄ›no
    #    - zkusÃ­me <h1> s class "h2 mb5 notranslate"
    #    - zkusÃ­me <tr> tabulku
    if details["ZnaÄka"] == "NezjiÅ¡tÄ›no" or details["Model"] == "NezjiÅ¡tÄ›no" or details["Rok"] == "NezjiÅ¡tÄ›no":
        h1_el = soup.find("h1", class_="h2 mb5 notranslate")
        if h1_el:
            span_el = h1_el.find("span", class_="regular")
            if span_el:
                span_text = span_el.get_text(" ", strip=True)
                parts = span_text.split(",")
                if len(parts) >= 2:
                    # "Model, 2018"
                    if details["Model"] == "NezjiÅ¡tÄ›no":
                        details["Model"] = parts[0].strip()
                    if details["Rok"] == "NezjiÅ¡tÄ›no":
                        details["Rok"] = parts[1].strip()
                elif len(parts) == 1 and details["Model"] == "NezjiÅ¡tÄ›no":
                    details["Model"] = parts[0].strip()

                brand_text = h1_el.get_text(" ", strip=True).replace(span_text, "").strip()
                if brand_text and details["ZnaÄka"] == "NezjiÅ¡tÄ›no":
                    details["ZnaÄka"] = brand_text

        # Fallback <tr>
        tr_tags = soup.find_all("tr")
        for tr in tr_tags:
            th = tr.find("th")
            td = tr.find("td")
            if th and td:
                header = th.get_text(" ", strip=True)
                value = td.get_text(" ", strip=True)
                if "Rok uvedenÃ­" in header and details["Rok"] == "NezjiÅ¡tÄ›no":
                    details["Rok"] = value
                elif "Tachometr" in header and details["NajetÃ© km"] == "NezjiÅ¡tÄ›no":
                    details["NajetÃ© km"] = value.replace("km", "").strip()
                elif "PÅ™evodovka" in header and details["PÅ™evodovka"] == "NezjiÅ¡tÄ›no":
                    details["PÅ™evodovka"] = value
                elif "Palivo" in header and details["Palivo"] == "NezjiÅ¡tÄ›no":
                    details["Palivo"] = value

    return details

def scrape_aaaauto_one_page(listing_url):
    print(f"ğŸ” Scrapuji strÃ¡nku: {listing_url}")
    links = get_listing_links(listing_url)
    print(f"ğŸ”— Nalezeno {len(links)} odkazÅ¯...")

    results = []
    with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:
        future_to_url = {executor.submit(parse_aaaauto_detail, link): link for link in links}
        for future in concurrent.futures.as_completed(future_to_url):
            url = future_to_url[future]
            try:
                data = future.result()
                results.append(data)
                print("------------------------------------------------------------")
                print(f"URL:        {data['URL']}")
                print(f"ZnaÄka:     {data['ZnaÄka']}")
                print(f"Model:      {data['Model']}")
                print(f"Rok:        {data['Rok']}")
                print(f"NajetÃ© km:  {data['NajetÃ© km']}")
                print(f"Cena:       {data['Cena']} KÄ")
                print(f"Palivo:     {data['Palivo']}")
                print(f"PÅ™evodovka: {data['PÅ™evodovka']}")
                print(f"VÃ½kon (kW): {data['VÃ½kon (kW)']}")
            except Exception as exc:
                print(f"âŒ Chyba pÅ™i zpracovÃ¡nÃ­ detailu {url}: {exc}")
    return results

def scrape_aaaauto(min_inzeraty=200, max_pages=20):
    base_url = "https://www.aaaauto.cz/ojete-vozy/"
    all_results = []
    page = 1
    while page <= max_pages:
        url = base_url if page == 1 else f"https://www.aaaauto.cz/ojete-vozy/#!&page={page}"
        print(f"\nğŸ“„ ZpracovÃ¡vÃ¡m strÃ¡nku {page}: {url}")
        page_results = scrape_aaaauto_one_page(url)
        if not page_results:
            print("ğŸ›‘ Å½Ã¡dnÃ© dalÅ¡Ã­ inzerÃ¡ty â€“ konÄÃ­m.")
            break
        all_results.extend(page_results)
        print(f"ğŸ“Š Celkem nasbÃ­rÃ¡no: {len(all_results)} inzerÃ¡tÅ¯.\n")
        if len(all_results) >= min_inzeraty:
            print(f"ğŸ¯ DosaÅ¾eno {min_inzeraty} inzerÃ¡tÅ¯ â€“ konÄÃ­m.")
            break
        page += 1
        time.sleep(1)

    df = pd.DataFrame(all_results)
    # OdstranÃ­me URL sloupec
    if "URL" in df.columns:
        df.drop(columns=["URL"], inplace=True)
    df.to_csv("auta_aaaauto.csv", index=False, encoding="utf-8-sig")
    print(f"\nğŸ’¾ UloÅ¾eno {len(df)} zÃ¡znamÅ¯ do 'auta_aaaauto.csv'.")
    return df

if __name__ == "__main__":
    df = scrape_aaaauto(min_inzeraty=200, max_pages=20)
    print("\nâœ… NÃ¡hled na prvnÃ­ch 5 Å™Ã¡dkÅ¯:")
    print(df.head())
>>>>>>> bb487f5469bd694df803453a80042a5bee964868
